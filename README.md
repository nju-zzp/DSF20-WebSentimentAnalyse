# 数据科学大作业——计算社会学 ：

### 一. 获取源数据

**目的**：**获取2019年12月8日至6月中旬**新浪、百度、天涯等网站，**有关疫情的新闻标题、内容以及重点新闻的评论**（重点新闻是指评论量远超（数据分布）其 它新闻的新闻）；获取央媒如荔枝网、新华网等网站12月8日-6月中旬有关 疫情的新闻标题、内容以及重点新闻的评论；

- 爬虫爬取微博及其他平台的新闻标题、正文，尤其是**评论**，作为源数据（最好包括评论数）

- 筛选出疫情相关的数据，结构化，保存为**json**格式
- （筛选出重点新闻：我觉得基本上都能算重点新闻吧，毕竟是央媒或者大V，评论数都很多，所以爬到数据后试试将评论可视化一下，看有没有必要进行筛选，我觉得评论数如果比较集中就可以不筛选）这一步骤我也有些迷茫，求交流



### 二. 建立语料库

1. 情绪分类：喜悦、乐观、嘲讽、焦虑、愤怒，并建立文件夹目录结构
2. 从源数据中选择部分作为训练集，利用程序遍历，将训练集每一条评论作为单一文本文件，打上的标签，并归类到各个类（放置进类别对应的文件夹）此时就建立好了语料库



### 三. 训练模型-情感预测 （HanLP库）

1. 创建朴素贝叶斯分类器，训练语料库，建立模型
2. 调用分类器，根据模型预测新的文本（测试集的文本）属于什么情感分类



### 四. 数据分析-可视化 （Pandas 和 Seaborn）

1. 各阶段选取相同的样本容量，在其中统计出各个阶段每类情绪的文本数量

2. 利用pandas和seaborn，绘制折线图/柱状图，进行分析并总结

   

-------------

@log：

v1.0：创建该文档，建立总体框架，全都是待解决的问题，其中首先是数据获取与结构化格式化的问题

​           by zzp in 1.21凌晨1:53

v2.0：完善该文档，填充了各个阶段的细节，待解决问题：数据获取，熟悉可视化接口

​           by zzp in 1.23 中午12:39
